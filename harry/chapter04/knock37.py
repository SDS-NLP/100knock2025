from collections import Counter
from knock36 import load_cleaned_articles
import MeCab
import os

def get_tokens_by_pos(articles, target_pos_prefix="åè©"):
    """
    æŒ‡å®šã—ãŸå“è©ï¼ˆä¾‹ï¼š"åè©"ï¼‰ã®åŸå½¢ã®ã¿ã‚’æŠ½å‡ºã—ã¦è¿”ã™
    è£œåŠ©è¨˜å·ã‚„è¨˜å·ã¯é™¤å¤–
    """
    unidic_path = os.path.expanduser("~/unidic-cwj")
    mecabrc_path = "/etc/mecabrc"

    mecab = MeCab.Tagger(
        f"-r {mecabrc_path} -d {unidic_path} "
        "-F '\\t%m\\t%f[7]\\t%f[6]\\t%f[23]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\t%f[8]\\t%f[9]\\t%f[12]\\t%f[28]\\n' "
        "--unk-format='\\t%m\\t\\t\\tæœªçŸ¥èª\\t\\t\\n'"
    )
    mecab.parse("")  # åˆæœŸåŒ–ãƒã‚°å¯¾ç­–

    tokens = []
    for article in articles:
        parsed = mecab.parse(article["text"])
        lines = parsed.strip().split("\n")

        for line in lines:
            if not line.strip() or line.startswith("EOS") or line.startswith("B"):
                continue

            parts = line.split()
            if len(parts) >= 5:
                surface = parts[0]
                lemma = parts[1]
                pos = parts[4]

                if (
                    target_pos_prefix in pos and
                    "è£œåŠ©è¨˜å·" not in pos and
                    "è¨˜å·" not in pos
                ):
                    tokens.append(lemma)  # åŸå½¢ã‚’ä½¿ã£ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    return tokens

if __name__ == "__main__":
    articles = load_cleaned_articles()
    words = get_tokens_by_pos(articles, target_pos_prefix="åè©-æ™®é€šåè©")
    counter = Counter(words)

    print("ğŸ”¢ åè©ï¼ˆæ™®é€šåè©ï¼‰ã®å‡ºç¾é »åº¦ ä¸Šä½20èªï¼š")
    for word, freq in counter.most_common(20):
        print(f"{word}\t{freq}")